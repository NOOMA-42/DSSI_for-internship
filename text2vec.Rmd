---
title: "text2vec"
author: "Paul"
date: "12/19/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(text2vec)
library(data.table)
library(glmnet)
library(tidyr)
library(tm)
library(stringr)
```


```{r}
train <- readRDS("clean_train.rds")
train %>% dim #with 1092 data

prep_fun = tolower
tok_fun = word_tokenizer

#create a token which the function prefixed with create can work with
it_train <- itoken(train$review_ext, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = train$id, 
             progressbar = FALSE)
#built the vocabulary with iterator
vocab = create_vocabulary(it_train)


#create dtm
vectorizer <- vocab_vectorizer(vocab)
t1 <- Sys.time()

dtm_train <- create_dtm(it_train, vectorizer)
dtm_train %>% rownames
print(difftime(Sys.time(), t1, units = 'sec'))
```


```{r}
#row equals to the # of doc, column equals to the # of the term
dim(dtm_train)

NFOLDS = 4
t1 = Sys.time()
glmnet_classifier <- cv.glmnet(x = dtm_train, y = train$type, 
                              family = 'multinomial', 
                              # L1 penalty
                              alpha = 1,
                              # interested in the area under ROC curve
                              type.measure = "mse",
                              
                              
                              
                              #F1 cross entropy
                              
                              
                              
                              # 5-fold cross-validation
                              nfolds = NFOLDS,
                              # high value is less accurate, but has faster training
                              thresh = 1e-3,
                              # again lower number of iterations for faster training
                              maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))

plot(glmnet_classifier)

m <- dtm_train %>% as.matrix() 
m %>% data.frame()


```

#GloVe
```{r}
prep_fun <- function(x){
    x <- tolower(x)
    str_remove_all(x, "\\.")
}
tok_fun <- word_tokenizer
# Create vocabulary. Terms will be unigrams (simple words).
it <- itoken(train$review_ext, 
            preprocessor = prep_fun, 
            tokenizer = tok_fun, 
            ids = train$id,
            progressbar = FALSE)
vocab <- create_vocabulary(it)
vocab <- prune_vocabulary(vocab, term_count_min = 5L)

# Use our filtered vocabulary
vectorizer <- vocab_vectorizer(vocab)
# use window of 5 for context words
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)

glove <- GlobalVectors$new(word_vectors_size = 50, vocabulary = vocab, x_max = 10)
wv_main <- glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)
wv_context <- glove$components
word_vectors <- wv_main + t(wv_context)
word_vectors %>% dim

rownames(dtm_train)
common_terms <- intersect(colnames(dtm_train), rownames(word_vectors))
dtm_averaged <-  normalize(dtm_train[, common_terms], "l1")
# you can re-weight dtm above with tf-idf instead of "l1" norm
sentence_vectors <- dtm_averaged %*% word_vectors[common_terms, ]


```


#ridge lasso 
```{r}
##ridge
n <- 1000 #sample
p <- 5000 #parameter
real_p <- 15 #only 15 of para help prediction, others are random noise

x <- matrix(rnorm(n*p), nrow=n, ncol=p) # a matrix full of noise
y <- apply(x[,1:real_p], 1, sum) + rnorm(n) #generate a set of outcome (which is sum of real_p here)

train_rows <- sample(1:n, .66*n)
x.train <- x[train_rows, ]
x.test <- x[-train_rows, ]
y.train <- y[train_rows, ]
y.test <- y[-train_rows, ]

alpha0.fit <- cv.glmnet(x.train, y.train, type.measure="mse", alpha=0, family="gaussian")
#cv means we wnat ot use cross validation to obtain optimal lambda (10 folds by default)

#type.measure cross validation
#if apply elastic-net to logistic we set type.measure to deviance

#family: gaussian => doing linear regression, binomial for logistic

#alpha0.fit: optimal values for lambda will be saved

alpha0.predicted <- predict(alpha0.fit, s=alpha0.fit$lambda.1se, newx=x.test)
#s: size of the penalty. optimal value for lambda, we could set s to lambda.min, in stat sense 1se is indistinguishable from lambda.1se but result in fewer para

#nexx to testing set

mean((y.test - alpha.predicted)^2)

##lasso

##elastic
#alpha = 0.5

####
#before we know lasso win, we needa try a lot
list.of.fits <- list()
for(i in 0:10){
    fit.name <- paste0("alpha", i/10)
    
    lsit.of.fits[[fit.name]] <- cv.glmnet(x.train, y.train, type.measure = "mse", alpha=i/10, family="gaussian")
}
results <- data.frame()
for(i in 0:10){
    fit.name <- paste0("alpha", i/10)
    
    predicted <- predict(list.of.fits[[fit.name]], s=list.of.fits[[fit.name]]$lambda.1se, newx=x.test)
    
    mean((y.test - alpha.predicted)^2)
    temp <- data.frame(alpha=i/10, mse=mse, fit.name=fit.name)
    results <- rbind(results, temp)
}
```

#xgboost
```{r}
#train$comment_text <- iconv(train$comment_text, 'UTF-8', 'ASCII')
corpus <- VCorpus(VectorSource(train$review_ext)) %>%
    tm_map(content_transformer(tolower)) %>%
    tm_map(removeNumbers) %>%
    tm_map(removePunctuation) %>%
    tm_map(removeWords, stopwords()) %>%
    tm_map(stemDocument) %>%
    tm_map(stripWhitespace)

dtm <- DocumentTermMatrix(corpus)



```

