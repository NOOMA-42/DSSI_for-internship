---
title: "AS04_scraping_json"
author: "YOUR_NAME"
date: "10/10/2019"
output:
  html_document:
    highlight: zenburn
    number_sections: no
    theme: cerulean
    toc: yes
    css: style.css
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Loading packages
```{r}
library(tidyverse)
library(httr)
library(jsonlite)
library(lubridate)
library(readr)
library(ggplot2)
options(stringsAsFactors = F)
```


# Scraping ubike data
- Here is the ubike json data link https://tcgbusfs.blob.core.windows.net/blobyoubike/YouBikeTP.json
- You are asked to scrape youbike data per 5 minutes, continuing at least 12 hours.
- Saving each ubike json per 5 minutes with file name containing timestamp, which means you should have at least 12*12 files

```{r hints}
# url <- "https://tcgbusfs.blob.core.windows.net/blobyoubike/YouBikeTP.json"
# GET(url) %>% content("text", encoding = "utf8") %>% cat(file = "./data/hw4/test.json")
# read_json("test.json") %>% class
# a <- read_lines("./data/hw4/test.json", locale = locale(encoding = "big5")) %>% fromJSON()


# for(i in 1:5){
#     message(i, "\t", now())
#     Sys.sleep(3)
# }

# Converting datetime to character
now() %>% format("%Y%m%d%H%M%S")

# Listing all files in a sub-folder
list.files("data/", ".*\\.rds") 

# If you pud your data in a sub-folder, you need to use full name to access them.
list.files("data/", ".*\\.rds", full.names = T) 

```
```{r}
# comment out scrapper
#
# for(i in 1:144){
#   url <- "https://tcgbusfs.blob.core.windows.net/blobyoubike/YouBikeTP.json"
#   location <- "./data/hw4/"
#   current_time <- now() %>% format("%Y%m%d%H%M%S")
#   GET(url) %>% 
#     content("text") %>% 
#     cat(file = str_c(location, "ubike_", current_time, ".json"))
#   message(i, "\t", "finished")
#   Sys.sleep(300)
# }
```


## **Q1.1 ANS** list files
- Using `list.files()` to list json files you scraped
- Using `length` to calculate `list.files()` result to see how many json files you have.

```{r}
# your code
fnames <- list.files("./data/hw4", pattern = ".*json", full.names = T)
length(fnames)
```

## Q1.2
- Reading JSON files one by one
- Converting them to data frame individually
- Defining one indicator: fullness = sbi/tot
- Calculating each site's fullness by time

```{r}
# your code
data <- data.frame()
for(i in 1:144){
  raw <- read_lines(fnames[i], locale = locale(encoding = "utf8")) %>% fromJSON()
  part <- raw$retVal %>% unlist() %>% matrix(ncol = 14, byrow=T) %>% as.data.frame()
  data <- rbind(data, part)
  message(i, "\t", "finished")
}
colnames(data) <-c("sno", "sna", "tot", "sbi", "sarea", "mday", "lat", "lng", "ar", "sareaen", "snaen", "aren", "bemp", "act")

# calculate fullness
data <- data %>%
  mutate(sbi = as.numeric(sbi), tot = as.numeric(tot)) %>%
  mutate(fullness = sbi/tot)
```


## **Q1.2 ANS**
- Using `geom_line` to display all site's fullness by time

```{r}
# your code
pic <- data.frame()
pic <- cbind(data[data$sno == '0001', ]$fullness, 1:144) %>% as.data.frame()
colnames(pic) <- c("fullness", "order")

ggplot(pic, aes(order, fullness)) + 
  geom_line()

```



# Q2.Scraping dcard forum (No extra point if you have solved the Q1 succesfully)
- (If you cannot solve Q1 successfully, try to solve the Q2)
- Selecting one chatting forum and Scraping at least 3 pages by for-loop (You must find out the rule of url formatting, and use for-loop to scrap at least 3 pages back)
- e.g., https://www.dcard.tw/f/relationship
- One video tutorial has introduced how to find out dcard forum's JSON data page by page.
- Adding code chunks as you need below

## **Q2.1.ANS** Print out class and dimension of your data
```{r}
# your code here
```


## **Q2.2.ANS** Using bar chart to show the number of post trend by week.
```{r}
# your code here
```



# (No extra point) Discovering one more website generated by JSON
- Try to find another website or webpage which is generated by JSON.
- Website title: [Fill-in-here]
- Website url: [Fill-in-here]
- Selecting one chatting forum and Scraping at least 3 pages by for-loop (You must find out the rule of url formatting, and use for-loop to scrap at least 3 pages back)
- Adding code chunks as you need below

## **Q3.ANS** Print out glimpse(), class, and dimension of your data
```{r}
# YOUR CODE SHOULD BE HERE
```

